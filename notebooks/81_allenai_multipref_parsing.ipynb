{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# OLMo Model Dataset Parsing\n",
                "\n",
                "This notebook loads and processes the OLMo dataset from Hugging Face for use with the ICAI framework."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install any missing packages\n",
                "# !pip install datasets transformers pandas numpy matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from datasets import load_dataset\n",
                "from tqdm.auto import tqdm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load the Multipref Dataset from Hugging Face\n",
                "\n",
                "The Allen Institute for AI (AI2) released the OLMo model along with several datasets. Let's load the main dataset from Hugging Face."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the OLMo dataset from Hugging Face\n",
                "# You can choose a specific config/subset if needed\n",
                "dataset = load_dataset(\"allenai/multipref\")\n",
                "ds_gpt4 = load_dataset(\"allenai/multipref\", \"gpt4_overall_binarized\")\n",
                "ds_human = load_dataset(\"allenai/multipref\", \"human_overall_binarized\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "main_df = dataset[\"train\"].to_pandas()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "main_df.to_csv(\"../data/processed/allenai/multipref_original.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get all unique values for overall_pref from the normal_worker_annotations\n",
                "overall_prefs = []\n",
                "for annotations in main_df[\"normal_worker_annotations\"]:\n",
                "    for annotation in annotations:\n",
                "        if 'overall_pref' in annotation:\n",
                "            overall_prefs.append(annotation['overall_pref'])\n",
                "\n",
                "# Display unique values and their counts\n",
                "unique_prefs = pd.Series(overall_prefs).value_counts()\n",
                "print(\"Unique overall_pref values:\")\n",
                "print(unique_prefs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pref_series = pd.Series(overall_prefs)\n",
                "num_a_better = (pref_series == \"A-is-slightly-better\").sum() + (pref_series == \"A-is-clearly-better\").sum()\n",
                "num_b_better = (pref_series == \"B-is-slightly-better\").sum() + (pref_series == \"B-is-clearly-better\").sum()\n",
                "num_tie = (pref_series == \"Tie\").sum()\n",
                "\n",
                "print(f\"Number of A-is-better: {num_a_better}\")\n",
                "print(f\"Number of B-is-better: {num_b_better}\")\n",
                "print(f\"Number of Tie: {num_tie}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# model distribution\n",
                "\n",
                "model_a_counts = main_df[\"model_a\"].value_counts()\n",
                "model_b_counts = main_df[\"model_b\"].value_counts()\n",
                "\n",
                "print(f\"Model A counts: {model_a_counts}\")\n",
                "print(f\"\\n\\nModel B counts: {model_b_counts}\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a function to flatten annotations and add them as columns to the main dataframe\n",
                "def flatten_annotations(df, annotation_column, prefix):\n",
                "    # Create a copy of the dataframe\n",
                "    result_df = df.copy()\n",
                "\n",
                "    # Get the maximum number of annotations in any row\n",
                "    max_annotations = max(len(annotations) for annotations in df[annotation_column])\n",
                "\n",
                "    # For each row in the dataframe\n",
                "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Flattening {prefix} annotations\"):\n",
                "        annotations = row[annotation_column]\n",
                "\n",
                "        # For each annotation in the list\n",
                "        for i, annotation in enumerate(annotations):\n",
                "            # Add each field as a new column with a prefix indicating the annotation source and index\n",
                "            for key, value in annotation.items():\n",
                "                column_name = f\"{prefix}_{i}_{key}\"\n",
                "\n",
                "                # Handle special case for arrays like 'helpful_checked_reasons'\n",
                "                if isinstance(value, np.ndarray):\n",
                "                    # Convert array to a comma-separated string\n",
                "                    value_str = ','.join(value) if len(value) > 0 else ''\n",
                "                    result_df.at[idx, column_name] = value_str\n",
                "\n",
                "                    # Also create individual boolean columns for each reason in the array\n",
                "                    for reason in value:\n",
                "                        reason_column = f\"{prefix}_{i}_{key}_{reason}\"\n",
                "                        result_df.at[idx, reason_column] = True\n",
                "                else:\n",
                "                    # For regular values, just add them directly\n",
                "                    result_df.at[idx, column_name] = value\n",
                "\n",
                "    return result_df\n",
                "\n",
                "# Flatten normal worker annotations\n",
                "main_df_with_normal = flatten_annotations(main_df, \"normal_worker_annotations\", \"normal\")\n",
                "\n",
                "# Flatten expert worker annotations\n",
                "main_df_flattened = flatten_annotations(main_df_with_normal, \"expert_worker_annotations\", \"expert\")\n",
                "\n",
                "# Display the number of columns before and after flattening\n",
                "print(f\"\\nOriginal number of columns: {len(main_df.columns)}\")\n",
                "print(f\"Number of columns after flattening: {len(main_df_flattened.columns)}\")\n",
                "\n",
                "# Display some of the new columns\n",
                "new_columns = [col for col in main_df_flattened.columns if col not in main_df.columns]\n",
                "print(f\"\\nNumber of new columns added: {len(new_columns)}\")\n",
                "print(\"\\nSample of new columns:\")\n",
                "print(new_columns[:20])\n",
                "\n",
                "main_df_flattened = main_df_flattened.sort_index(axis=1)\n",
                "\n",
                "# Example of what an annotation looks like (for reference)\n",
                "# example_evaluator = {'evaluator': 'keen_williams',\n",
                "#  'harmless_checked_reasons': np.array([], dtype=object),\n",
                "#  'harmless_confidence': 'absolutely-confident',\n",
                "#  'harmless_own_reason': '',\n",
                "#  'harmless_pref': 'Tie',\n",
                "#  'helpful_checked_reasons': np.array(['well_formatted', 'coherent', 'creative', 'better_tone'],\n",
                "#        dtype=object),\n",
                "#  'helpful_confidence': 'absolutely-confident',\n",
                "#  'helpful_own_reason': '',\n",
                "#  'helpful_pref': 'B-is-clearly-better',\n",
                "#  'overall_confidence': 'absolutely-confident',\n",
                "#  'overall_pref': 'B-is-clearly-better',\n",
                "#  'time_spent': 213,\n",
                "#  'timestamp': '2024-05-03 04:17:06.661562',\n",
                "#  'truthful_checked_reasons': np.array([], dtype=object),\n",
                "#  'truthful_confidence': 'absolutely-confident',\n",
                "#  'truthful_own_reason': '',\n",
                "#  'truthful_pref': 'Tie'}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "# merge\n",
                "\n",
                "df_gpt4 = ds_gpt4[\"train\"].to_pandas()\n",
                "df_human = ds_human[\"train\"].to_pandas()\n",
                "\n",
                "# add suffixes to columns\n",
                "df_gpt4 = df_gpt4.add_suffix(\"_gpt4\")\n",
                "df_human = df_human.add_suffix(\"_human\")\n",
                "\n",
                "\n",
                "merged_df = pd.merge(main_df_flattened, df_gpt4, left_on=\"comparison_id\", right_on=\"comparison_id_gpt4\", how=\"left\")\n",
                "merged_df = pd.merge(merged_df, df_human, left_on=\"comparison_id\", right_on=\"comparison_id_human\", how=\"left\")\n",
                "\n",
                "\n",
                "for dataset in [ds_gpt4, ds_human]:\n",
                "    for idx, row in merged_df.iterrows():\n",
                "        for chat_col in [\"gpt4\", \"human\"]:\n",
                "            selected_completion = row[f\"chosen_{chat_col}\"][-1][\"content\"]\n",
                "            if selected_completion == row[\"completion_a\"]:\n",
                "                merged_df.at[idx, f\"preferred_text_{chat_col}\"] = \"text_a\"\n",
                "            elif selected_completion == row[\"completion_b\"]:\n",
                "                merged_df.at[idx, f\"preferred_text_{chat_col}\"] = \"text_b\"\n",
                "            else:\n",
                "                print(f\"Selected completion {selected_completion} not in {row['completion_a']} or {row['completion_b']}\")\n",
                "                merged_df.at[idx, f\"preferred_text_{chat_col}\"] = \"invalid\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "merged_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create text_a and text_b columns from \"text\" and \"completion_a\" and \"completion_b\"\n",
                "# e.g. text_a = [{'role': 'user', 'content': text}, {'role': 'assistant', 'content': completion_a}]\n",
                "\n",
                "merged_df[\"text_a\"] = merged_df[[\"text\", \"completion_a\"]].apply(lambda x: [{'role': 'user', 'content': x[\"text\"]}, {'role': 'assistant', 'content': x[\"completion_a\"]}], axis=1)\n",
                "merged_df[\"text_b\"] = merged_df[[\"text\", \"completion_b\"]].apply(lambda x: [{'role': 'user', 'content': x[\"text\"]}, {'role': 'assistant', 'content': x[\"completion_b\"]}], axis=1)\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(merged_df[\"text_a\"][0])\n",
                "print(merged_df[\"text_b\"][0])\n",
                "print(merged_df[\"chosen_gpt4\"][0])\n",
                "print(merged_df[\"chosen_human\"][0])\n",
                "print(merged_df[\"preferred_text_gpt4\"][0])\n",
                "print(merged_df[\"preferred_text_human\"][0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "list(merged_df.columns)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert normal and expert overall preferences into \"text_a\", \"text_b\" or \"tie\"\n",
                "# First, check if these columns exist in the dataframe\n",
                "normal_pref_cols = [col for col in merged_df.columns if 'normal_' in col and '_overall_pref' in col]\n",
                "expert_pref_cols = [col for col in merged_df.columns if 'expert_' in col and '_overall_pref' in col]\n",
                "\n",
                "# Function to convert preference notation to text_a/text_b/tie\n",
                "def convert_pref_to_text(pref):\n",
                "    if pref == 'A-is-slightly-better' or pref == 'A-is-clearly-better':\n",
                "        return 'text_a'\n",
                "    elif pref == 'B-is-slightly-better' or pref == 'B-is-clearly-better':\n",
                "        return 'text_b'\n",
                "    elif pref == 'Tie':\n",
                "        return 'tie'\n",
                "    else:\n",
                "        return 'invalid'\n",
                "\n",
                "# Process normal annotator preferences\n",
                "for col in normal_pref_cols:\n",
                "    new_col = col.replace('_overall_pref', '_preferred_text')\n",
                "    merged_df[new_col] = merged_df[col].apply(convert_pref_to_text)\n",
                "\n",
                "# Process expert annotator preferences\n",
                "for col in expert_pref_cols:\n",
                "    new_col = col.replace('_overall_pref', '_preferred_text')\n",
                "    merged_df[new_col] = merged_df[col].apply(convert_pref_to_text)\n",
                "\n",
                "print(f\"Converted {len(normal_pref_cols)} normal annotator preferences and {len(expert_pref_cols)} expert annotator preferences\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "# remove \"prompt_human\" and \"prompt_gpt4\" columns\n",
                "# remove \"rejected_gpt4\" and \"rejected_human\" columns\n",
                "# remove \"chosen_gpt4\" and \"chosen_human\" columns\n",
                "\n",
                "merged_df = merged_df.drop(columns=[\"prompt_human\", \"prompt_gpt4\", \"rejected_gpt4\", \"rejected_human\", \"chosen_gpt4\", \"chosen_human\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "merged_df[\"preferred_text\"] = merged_df[\"preferred_text_human\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "merged_df[:10000].to_csv(\"../data/processed/allenai/multipref_gpt4_human_merged.csv\", index=True, index_label=\"index\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "merged_df.iloc[0].to_dict()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
